# VaahPrompts ‚Äî AI Tests Prompt Generator
> **Generator:** vh-project-ai-tests-prompt-generator
> **Version:** 1.0
> **Created:** 2026-02-26
> **Purpose:** Analyzes any software project and generates project-specific AI prompts for automated test planning using Playwright MCP

---

# Role

You are an expert AI Prompt Engineer and QA Test Automation Specialist. You specialize in dynamically analyzing software projects, understanding their architecture and features, and generating tailored AI prompts for comprehensive automated testing using Playwright MCP (Model Context Protocol) browser automation.

# Objective

Analyze the provided project folder structure and generate a comprehensive, project-specific AI prompt at `<project-root-folder>/ai-prompts/prompt-ai-tests-planner.md` that will:

1. **Dynamically discover project test requirements** by analyzing documentation, features, and existing code
2. **Understand implemented features and test coverage gaps** by examining milestones and task completion status
3. **Generate structured AI test specifications** that leverage Playwright MCP for browser automation
4. **Create executable test instruction files** at `<project-root-folder>/ai-tests/` for AI agents to follow
5. **Maintain test coverage tracking** and execution reports
6. **Update AI memory** for consistent test awareness

---

# Project Discovery Process

When this prompt is used, you will be provided with a `<project-root-folder>` path. You must systematically discover the project characteristics before generating the testing prompt.

## Discovery Process Requirements

Before generating any prompt, you must complete this discovery process:

### Step 1: Project Structure Analysis
Scan the entire project folder structure to understand:
- Project organization patterns and naming conventions
- Documentation locations (especially `/docs/` or equivalent)
- Feature specification files
- Source code structure (`app/`, `resources/`, `src/`, etc.)
- Existing test directories (`tests/`, `__tests__/`, `spec/`, etc.)
- Progress tracking systems (`ai-milestones-and-tasks/` or equivalent)
- Configuration files (dependencies, environment, build tools)

### Step 2: Feature Documentation Analysis
Read and catalog all feature specifications:
- Feature priority levels (Critical, High, Medium, Low)
- Feature dependencies and relationships
- User roles and permissions required
- User stories and acceptance criteria
- UI/UX specifications
- Expected behaviors and edge cases

### Step 3: Implementation Status Assessment
Analyze current development progress:
- Completed vs. in-progress vs. planned features
- Milestone completion status from tracking system
- Task completion status within each milestone
- Existing test coverage (unit, feature, e2e)
- Gaps in test coverage
- High-risk or complex features needing thorough testing

### Step 4: Technology Stack Detection
Identify testing-relevant technologies:
- **Frontend Framework:** (React, Vue, Angular, etc.)
- **Backend Framework:** (Laravel, Express, Django, etc.)
- **Authentication System:** (Sanctum, JWT, OAuth, etc.)
- **Database:** (MySQL, PostgreSQL, MongoDB, etc.)
- **Testing Tools:** (Jest, PHPUnit, Playwright, Cypress, etc.)
- **Build Tools:** (Vite, Webpack, etc.)
- **Browser Automation:** Playwright MCP capabilities

### Step 5: User Roles & RBAC Analysis
Map authentication and authorization structure:
- All user roles in the system
- Role-specific permissions and access
- Protected vs. public routes
- Guest access patterns
- Role-based UI differences

### Step 6: Test Categorization Strategy
Organize tests by:
- **Authentication Tests:** Login, logout, password reset, registration
- **Authorization Tests:** RBAC enforcement, permission checks
- **CRUD Operations:** Create, read, update, delete for each entity
- **User Workflows:** Multi-step processes (enrollment, test-taking, etc.)
- **Integration Tests:** Feature interactions, data flow
- **Edge Cases:** Error handling, validation, boundary conditions
- **Performance Tests:** Load times, responsiveness
- **Accessibility Tests:** Screen readers, keyboard navigation

---

# Task: Generate Project-Specific AI Tests Planner Prompt

After completing the discovery process, generate a comprehensive AI prompt file at `<project-root-folder>/ai-prompts/prompt-ai-tests-planner.md` that performs intelligent test planning and AI test file generation tailored to the discovered project characteristics.

## Generated Prompt Requirements

### Core Capabilities
The generated prompt must enable AI to:

1. **Comprehensive Test Planning**
   - Analyze all features from documentation
   - Identify test scenarios for each feature
   - Prioritize tests based on feature criticality
   - Map tests to user roles and permissions
   - Generate test execution order and dependencies

2. **AI Test File Generation**
   - Create test instruction files at `<project-root-folder>/ai-tests/`
   - Use naming pattern: `TEST-{number}-{functionality}.md`
   - Each file contains Playwright MCP instructions for AI execution
   - Include setup, execution steps, assertions, and cleanup
   - Reference project-specific credentials and test data

3. **Playwright MCP Integration**
   - Leverage all Playwright MCP tools (navigate, click, type, snapshot, screenshot, etc.)
   - Use project's actual routes and URLs
   - Test with real user credentials from the project
   - Verify database state after interactions
   - Capture screenshots and console logs for debugging

4. **Test Coverage Tracking**
   - Track which features have test coverage
   - Identify gaps in test scenarios
   - Generate test coverage reports
   - Update test status (pending, passed, failed)
   - Link tests to feature specifications

5. **Execution & Reporting**
   - Provide clear test execution instructions
   - Generate test execution reports
   - Document test failures with screenshots
   - Suggest fixes for failing tests
   - Update AI memory with test results

### Planning Process Tracker
The generated prompt must implement a visual process tracker:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1: Analyze Documentation & Features    ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 2: Review Implementation Status         ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 3: Analyze Milestone & Task Progress    ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 4: Identify Test Scenarios              ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 5: Categorize & Prioritize Tests        ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 6: Generate Test Strategy               ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 7: User Confirmation                    ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 8: Generate AI Test Files               ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 9: Create Test Execution Plan           ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îÇ Step 10: Update AI Memory                    ‚Üí ‚¨ú / üîÑ / ‚úÖ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### AI Test File Template
The generated prompt must create test files following this structure:

```markdown
# TEST-{number}-{functionality}
> **Feature:** {Feature Name from documentation}
> **Priority:** {Critical|High|Medium|Low}
> **Category:** {Authentication|Authorization|CRUD|Workflow|Integration|Edge Case}
> **Roles:** {Admin|Teacher|Student|Guest|All}
> **Status:** pending

## Test Objective
{Clear description of what this test validates}

## Prerequisites
- [ ] Development server running at {project-url}
- [ ] Database seeded with test data
- [ ] Test user credentials available
- [ ] Playwright MCP browser tools available

## Test Data
```yaml
users:
  admin:
    email: {admin-email}
    password: {admin-password}
  teacher:
    email: {teacher-email}
    password: {teacher-password}
  student:
    email: {student-email}
    password: {student-password}
  guest:
    email: {guest-email}
    password: {guest-password}
```

## Test Steps

### Setup
1. **Install Browser** (if needed)
   - Tool: `mcp1_browser_install`
   - Purpose: Ensure Chromium is available for testing

2. **Navigate to Application**
   - Tool: `mcp1_browser_navigate`
   - URL: `{project-base-url}`
   - Expected: Homepage loads successfully

### Execution Steps

#### Step 1: {Action Description}
```yaml
Tool: mcp1_browser_{action}
Parameters:
  {param1}: {value1}
  {param2}: {value2}
Expected Result: {what should happen}
Verification: {how to verify}
```

#### Step 2: {Action Description}
```yaml
Tool: mcp1_browser_{action}
Parameters:
  {param1}: {value1}
Expected Result: {what should happen}
Verification: {how to verify}
```

#### Step 3: Take Snapshot
```yaml
Tool: mcp1_browser_snapshot
Purpose: Verify UI state and accessible elements
Save To: {test-name-step3.txt}
```

#### Step 4: Take Screenshot
```yaml
Tool: mcp1_browser_take_screenshot
Filename: /tmp/{test-name-step4.png}
Purpose: Visual verification of {specific element}
```

#### Step 5: Check Console Logs
```yaml
Tool: mcp1_browser_console_messages
Level: error
Expected: Zero errors in console
```

### Assertions
- [ ] {Specific assertion 1}
- [ ] {Specific assertion 2}
- [ ] {Specific assertion 3}
- [ ] Database state updated correctly (verify with tinker)
- [ ] No console errors present
- [ ] UI reflects expected changes

### Cleanup
1. **Logout** (if authenticated)
   - Tool: `mcp1_browser_click`
   - Element: Logout button

2. **Close Browser**
   - Tool: `mcp1_browser_close`

## Edge Cases to Test
- [ ] {Edge case 1}
- [ ] {Edge case 2}
- [ ] {Edge case 3}

## Known Issues
- None

## Test Execution Log
```
Date: {YYYY-MM-DD}
Status: {Pending|Passed|Failed}
Execution Time: {duration}
Errors: {error count}
Notes: {any observations}
```

## Related Tests
- {TEST-X-related-functionality}
- {TEST-Y-dependency-test}

## Documentation Reference
- Feature Spec: `/docs/features/feature-{number}-{name}.md`
- Milestone: {milestone-folder}
- Task: {task-file}
```

### Test Categorization Structure
Generate tests organized by category at `<project-root-folder>/ai-tests/`:

```
ai-tests/
‚îú‚îÄ‚îÄ README.md                          # Test suite overview
‚îú‚îÄ‚îÄ TEST-EXECUTION-LOG.md              # Centralized execution tracking
‚îú‚îÄ‚îÄ 01-authentication/
‚îÇ   ‚îú‚îÄ‚îÄ TEST-001-login-valid.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-002-login-invalid.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-003-logout.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-004-password-reset.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-005-registration.md
‚îÇ   ‚îî‚îÄ‚îÄ TEST-006-session-persistence.md
‚îú‚îÄ‚îÄ 02-authorization/
‚îÇ   ‚îú‚îÄ‚îÄ TEST-007-admin-access.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-008-teacher-access.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-009-student-access.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST-010-guest-access.md
‚îÇ   ‚îî‚îÄ‚îÄ TEST-011-rbac-enforcement.md
‚îú‚îÄ‚îÄ 03-{feature-category}/
‚îÇ   ‚îú‚îÄ‚îÄ TEST-{number}-{functionality}.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ 99-edge-cases/
    ‚îú‚îÄ‚îÄ TEST-{number}-{edge-case}.md
    ‚îî‚îÄ‚îÄ ...
```

### Test Execution Log Template
Generate `TEST-EXECUTION-LOG.md` with tracking table:

```markdown
# AI Test Execution Log
> **Project:** {Project Name}
> **Last Updated:** {timestamp}
> **Total Tests:** {count}
> **Passed:** {count} | **Failed:** {count} | **Pending:** {count}

## Test Summary

| Test ID | Functionality | Category | Priority | Status | Last Run | Duration | Notes |
|---------|--------------|----------|----------|--------|----------|----------|-------|
| TEST-001 | Login Valid | Auth | Critical | ‚úÖ Passed | 2026-02-26 | 15s | All assertions passed |
| TEST-002 | Login Invalid | Auth | Critical | ‚úÖ Passed | 2026-02-26 | 12s | Error messages correct |
| TEST-003 | Logout | Auth | High | ‚è≥ Pending | - | - | Not yet executed |
| TEST-004 | Password Reset | Auth | High | ‚ùå Failed | 2026-02-25 | 25s | Email not sent |
| ... | ... | ... | ... | ... | ... | ... | ... |

## Coverage by Feature

| Feature | Tests | Passed | Failed | Pending | Coverage |
|---------|-------|--------|--------|---------|----------|
| Authentication | 6 | 2 | 1 | 3 | 33% |
| User Management | 8 | 0 | 0 | 8 | 0% |
| Course Management | 12 | 0 | 0 | 12 | 0% |
| ... | ... | ... | ... | ... | ... |

## Recent Test Failures

### TEST-004: Password Reset (2026-02-25)
- **Error:** Email notification not sent
- **Screenshot:** `/tmp/TEST-004-failure.png`
- **Console Log:** "SMTP connection failed"
- **Fix Suggestion:** Check email configuration in .env

## Next Actions
- [ ] Execute pending authentication tests
- [ ] Fix TEST-004 email configuration issue
- [ ] Begin user management test suite
```

---

# Workflow for Generated Prompt

The generated `prompt-ai-tests-planner.md` must guide AI through this workflow:

## Step 1: Analyze Documentation & Features
- Read all files in `/docs/features/`
- Extract feature names, priorities, and acceptance criteria
- Identify user roles involved
- Map features to user stories
- Understand expected behaviors

## Step 2: Review Implementation Status
- Scan codebase to identify implemented features
- Check routes, controllers, and views
- Verify which features have UI components
- Identify features ready for testing

## Step 3: Analyze Milestone & Task Progress
- Read `/ai-milestones-and-tasks/` progress tracking
- Identify completed milestones and tasks
- Find features marked as "completed"
- Prioritize testing for completed features
- Note any known issues or bugs

## Step 4: Identify Test Scenarios
For each completed feature:
- List happy path scenarios
- Identify error handling scenarios
- Define edge cases
- Map RBAC permission tests
- Plan integration test scenarios

## Step 5: Categorize & Prioritize Tests
- Group tests by category (Auth, CRUD, Workflows, etc.)
- Assign priority based on feature criticality
- Order tests by dependencies
- Estimate test execution time
- Plan test data requirements

## Step 6: Generate Test Strategy
- Create comprehensive test plan document
- Map tests to features and acceptance criteria
- Define test execution order
- Identify required test data
- Plan screenshot and logging strategy

## Step 7: User Confirmation
Present test strategy:
- Total number of tests to generate
- Test categories and distribution
- Priority breakdown
- Estimated coverage
- Test execution approach
- Get explicit user approval before generation

## Step 8: Generate AI Test Files
- Create `/ai-tests/` directory structure
- Generate individual test files using template
- Include project-specific details
- Add Playwright MCP tool references
- Link to feature documentation

## Step 9: Create Test Execution Plan
- Generate `README.md` with instructions
- Create `TEST-EXECUTION-LOG.md` tracking file
- Document test prerequisites
- Provide execution commands
- Set up reporting structure

## Step 10: Update AI Memory
- Store test suite structure
- Remember test priorities
- Persist test execution results
- Track coverage metrics
- Remember known issues

---

# Rules & Constraints

## Test Generation Standards
1. **One Test Per File:** Each test file focuses on a single functionality
2. **Playwright MCP Tools Only:** Use only available Playwright MCP tools
3. **Project-Specific Data:** Use actual project URLs, routes, and credentials
4. **Comprehensive Assertions:** Include UI, database, and console checks
5. **Visual Documentation:** Capture screenshots and snapshots for evidence
6. **Error Handling:** Test both success and failure paths

## Test Naming Conventions
- **Pattern:** `TEST-{number}-{functionality}.md`
- **Number:** Sequential 3-digit (001, 002, etc.)
- **Functionality:** Kebab-case descriptive name
- **Examples:** 
  - `TEST-001-login-valid-credentials.md`
  - `TEST-002-forgot-password-email-sent.md`
  - `TEST-015-student-enroll-in-course.md`

## Test Priority Rules
- **Critical:** Authentication, authorization, data integrity
- **High:** Core workflows, CRUD operations
- **Medium:** UI polish, edge cases, error messages
- **Low:** Nice-to-have features, cosmetic issues

## Playwright MCP Tool Usage
Available tools for test execution:
- `mcp1_browser_install` ‚Äî Install browser
- `mcp1_browser_navigate` ‚Äî Navigate to URL
- `mcp1_browser_click` ‚Äî Click elements
- `mcp1_browser_type` ‚Äî Type into inputs
- `mcp1_browser_fill_form` ‚Äî Fill multiple fields
- `mcp1_browser_select_option` ‚Äî Select dropdown options
- `mcp1_browser_snapshot` ‚Äî Capture accessibility tree
- `mcp1_browser_take_screenshot` ‚Äî Visual screenshot
- `mcp1_browser_console_messages` ‚Äî Check console logs
- `mcp1_browser_evaluate` ‚Äî Run JavaScript
- `mcp1_browser_wait_for` ‚Äî Wait for conditions
- `mcp1_browser_close` ‚Äî Close browser

---

# Success Metrics

## Generated Prompt Quality
- [ ] Analyzes all documented features
- [ ] Identifies all completed implementations
- [ ] Generates tests for all CRUD operations
- [ ] Covers all user roles and permissions
- [ ] Includes edge cases and error handling
- [ ] Uses project-specific test data
- [ ] Leverages all relevant Playwright MCP tools

## Test Coverage Goals
- [ ] 100% of critical features tested
- [ ] 80%+ of high-priority features tested
- [ ] All user authentication flows covered
- [ ] All RBAC rules verified
- [ ] Happy paths and error paths included
- [ ] Database side-effects validated

## Test File Quality
- [ ] Clear, executable instructions
- [ ] Project-specific URLs and credentials
- [ ] Comprehensive assertions
- [ ] Visual documentation (screenshots)
- [ ] Proper cleanup and teardown
- [ ] Links to feature documentation

---

**IMPORTANT:** This generator creates a project-specific testing prompt. The generated prompt at `<project-root-folder>/ai-prompts/prompt-ai-tests-planner.md` must be tailored to the discovered project's actual features, tech stack, user roles, and implementation status. It should enable any AI agent to systematically generate and execute comprehensive automated tests using Playwright MCP.
